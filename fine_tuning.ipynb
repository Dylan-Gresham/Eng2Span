{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdnY_UsJtnEr"
   },
   "source": [
    "# English to Spanish Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the Hugging Face Translation Tutorial (https://huggingface.co/learn/nlp-course/en/chapter7/4#preparing-the-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IPwytiHxtnEt",
    "outputId": "aa9a8296-878e-48bd-aa15-90ec4e0cb23d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# hugging face translation tutorial\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "# custom training loop\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# customize model\n",
    "from transformers import AutoModel, PretrainedConfig, AutoConfig\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2025-03-21\n",
    "* May be interested in using Subword Segmentation on rare/novel words?\n",
    "* May be interested in using a different dataset than `helsinski-NLP/kde4`\n",
    "  * Weird errors like english words attached to end of spanish translations\n",
    "  * Small sentences, maybe this was a textbook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Restart the kernel every time you have to download something..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tf-keras --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch] OR !pip install 'accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Helinski-NLP/kde4 dataset (https://huggingface.co/datasets/Helsinki-NLP/kde4)\n",
    "# this dataset has a good amount of oddness/errors, we should probably consider something else\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"es\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XftcI9ItnEu",
    "outputId": "2f51ff9e-8aaf-4321-e2d7-a2d7e13a8285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 218655\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0T5M3sGtnEv",
    "outputId": "6b0c1227-ac40-4f84-a748-511ef5e466f7"
   },
   "outputs": [],
   "source": [
    "# split again for validation set\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Left Arrow', 'es': 'Tecla de dirección izquierda.'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][84][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Edit and paint images', 'es': 'Editar y pintar imágenesName'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][22][\"translation\"] # interesting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Removes the selected render mode.',\n",
       " 'es': 'Elimina el modo de procesado seleccionado.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][66][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23292, 9, 5, 4836, 17058, 9888, 3, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'labels': [47448, 14, 1160, 4, 38117, 15569, 3, 0]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][66][\"translation\"][\"en\"]\n",
    "es_sentence = split_datasets[\"train\"][66][\"translation\"][\"es\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=es_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Eli', 'mina', '▁el', '▁mod', 'o', '▁de', '▁pro', 'ces', 'ado', '▁s', 'ele', 'c', 'cion', 'ado', '.', '</s>']\n",
      "['▁Elimina', '▁el', '▁modo', '▁de', '▁procesado', '▁seleccionado', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(es_sentence) # english tokenizer preprocessing spanish sentence, does not do well (this is demo stuff, just to make sure the tokenizer is applied correctly)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128 # depends on how long text is... text does not seeem to be very long?\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"es\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ We don’t pay attention to the attention mask of the targets, as the model won’t expect it. Instead, the labels corresponding to a padding token should be set to -100 so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIdySeMItnEv"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '¿Has considerado el caballo?'}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demo\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Have you considered horse?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collation\n",
    "Deals with padding for dynamic batching. Need to pad labels to maximum length.\n",
    "\n",
    "I do not know what this means beyond that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3244,   755,     0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100],\n",
       "        [  664,  1532, 12100,    19, 17220,  6533,  2834,  1257, 15569,     3,\n",
       "             0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65000,  3244,   755,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n",
       "         65000],\n",
       "        [65000,   664,  1532, 12100,    19, 17220,  6533,  2834,  1257, 15569,\n",
       "             3]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3244, 755, 0]\n",
      "[664, 1532, 12100, 19, 17220, 6533, 2834, 1257, 15569, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# labels for first and second elements in dataset\n",
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits :eyes:\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them (these are padding labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning - Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(AutoModelForSeq2SeqLM): # not sure if this is the correct type, but here we go # AutoModelForSeq2SeqLM\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # I'm not sure if this,,, creates another set of layers on top?\n",
    "        self.l1 = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 256)\n",
    "        # self.classifier = torch.nn.Linear(768, NUM_OUT)\n",
    "#         self.dropout = torch.nn.Dropout(0.5)\n",
    "        # self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.l1.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "# #         pooler = self.pre_classifier(pooler)\n",
    "# #         pooler = torch.nn.Tanh()(pooler)\n",
    "# #         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier(pooler)\n",
    "#         output = self.softmax(output)\n",
    "        # return output\n",
    "\n",
    "        # output = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return output #.logits\n",
    "\n",
    "    def get_logits(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.l1.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG(PretrainedConfig):\n",
    "    model_type = \"our-model\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        d_model=512,\n",
    "        d_kv=64,\n",
    "        d_ff=2048,\n",
    "        num_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        num_heads=8,\n",
    "        relative_attention_num_buckets=32,\n",
    "        relative_attention_max_distance=128,\n",
    "        dropout_rate=0.1,\n",
    "        layer_norm_epsilon=1e-6,\n",
    "        initializer_factor=1.0,\n",
    "        feed_forward_proj=\"relu\",\n",
    "        use_cache=True,\n",
    "        pad_token_id=0,\n",
    "        eos_token_id=1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_kv = d_kv\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.relative_attention_num_buckets = relative_attention_num_buckets\n",
    "        self.relative_attention_max_distance = relative_attention_max_distance\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_factor = initializer_factor\n",
    "        self.feed_forward_proj = feed_forward_proj\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        super().__init__(pad_token_id=pad_token_id, eos_token_id=eos_token_id, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"our-model\", CONFIG)\n",
    "AutoModel.register(CONFIG, MODEL)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model = MODEL.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to huggingface hubm (needs login)\n",
    "# from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "# model_name = \"marian-finetuned-kde4-en-to-es-accelerate\"\n",
    "# repo_name = get_full_repo_name(model_name)\n",
    "# output_dir = \"marian-finetuned-kde4-en-to-es-accelerate\"\n",
    "# repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21f0f89b55a4b3086379607f403ad49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    # accelerator.wait_for_everyone()\n",
    "    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    # if accelerator.is_main_process:\n",
    "    #     tokenizer.save_pretrained(output_dir)\n",
    "    #     repo.push_to_hub(\n",
    "    #         commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Fine Tuning - Pre Defined Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into hugging face here:\n",
    "```py\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "```\n",
    "OR\n",
    "```sh\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-es\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # speed up training on modern GPUs\n",
    "    push_to_hub=False, # I don't know how to do this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# score original model as baseline to ensure fine tuning actually improves\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tqdm(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:197\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\trainer.py:4105\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4102\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4104\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4106\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4115\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\trainer.py:4299\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4296\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4299\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4300\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4301\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4303\u001b[0m )\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:352\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels:\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m--> 352\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoother \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoother(outputs, inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1403\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1399\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1400\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1401\u001b[0m         )\n\u001b[1;32m-> 1403\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1420\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1422\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1195\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1189\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1190\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1192\u001b[0m     )\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1195\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:995\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    982\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    983\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    984\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    992\u001b[0m         use_cache,\n\u001b[0;32m    993\u001b[0m     )\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 995\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m\\\\?\\C:\\Users\\steal\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:441\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[0;32m    440\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 441\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# score original model as baseline to ensure fine tuning actually improves\n",
    "trainer.evaluate(max_length=max_length) # takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_length) # takes time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Demo with Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-es\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenizer Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NllbTokenizerFast(name_or_path='facebook/nllb-200-distilled-600M', vocab_size=256204, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['ace_Arab', 'ace_Latn', 'acm_Arab', 'acq_Arab', 'aeb_Arab', 'afr_Latn', 'ajp_Arab', 'aka_Latn', 'amh_Ethi', 'apc_Arab', 'arb_Arab', 'ars_Arab', 'ary_Arab', 'arz_Arab', 'asm_Beng', 'ast_Latn', 'awa_Deva', 'ayr_Latn', 'azb_Arab', 'azj_Latn', 'bak_Cyrl', 'bam_Latn', 'ban_Latn', 'bel_Cyrl', 'bem_Latn', 'ben_Beng', 'bho_Deva', 'bjn_Arab', 'bjn_Latn', 'bod_Tibt', 'bos_Latn', 'bug_Latn', 'bul_Cyrl', 'cat_Latn', 'ceb_Latn', 'ces_Latn', 'cjk_Latn', 'ckb_Arab', 'crh_Latn', 'cym_Latn', 'dan_Latn', 'deu_Latn', 'dik_Latn', 'dyu_Latn', 'dzo_Tibt', 'ell_Grek', 'eng_Latn', 'epo_Latn', 'est_Latn', 'eus_Latn', 'ewe_Latn', 'fao_Latn', 'pes_Arab', 'fij_Latn', 'fin_Latn', 'fon_Latn', 'fra_Latn', 'fur_Latn', 'fuv_Latn', 'gla_Latn', 'gle_Latn', 'glg_Latn', 'grn_Latn', 'guj_Gujr', 'hat_Latn', 'hau_Latn', 'heb_Hebr', 'hin_Deva', 'hne_Deva', 'hrv_Latn', 'hun_Latn', 'hye_Armn', 'ibo_Latn', 'ilo_Latn', 'ind_Latn', 'isl_Latn', 'ita_Latn', 'jav_Latn', 'jpn_Jpan', 'kab_Latn', 'kac_Latn', 'kam_Latn', 'kan_Knda', 'kas_Arab', 'kas_Deva', 'kat_Geor', 'knc_Arab', 'knc_Latn', 'kaz_Cyrl', 'kbp_Latn', 'kea_Latn', 'khm_Khmr', 'kik_Latn', 'kin_Latn', 'kir_Cyrl', 'kmb_Latn', 'kon_Latn', 'kor_Hang', 'kmr_Latn', 'lao_Laoo', 'lvs_Latn', 'lij_Latn', 'lim_Latn', 'lin_Latn', 'lit_Latn', 'lmo_Latn', 'ltg_Latn', 'ltz_Latn', 'lua_Latn', 'lug_Latn', 'luo_Latn', 'lus_Latn', 'mag_Deva', 'mai_Deva', 'mal_Mlym', 'mar_Deva', 'min_Latn', 'mkd_Cyrl', 'plt_Latn', 'mlt_Latn', 'mni_Beng', 'khk_Cyrl', 'mos_Latn', 'mri_Latn', 'zsm_Latn', 'mya_Mymr', 'nld_Latn', 'nno_Latn', 'nob_Latn', 'npi_Deva', 'nso_Latn', 'nus_Latn', 'nya_Latn', 'oci_Latn', 'gaz_Latn', 'ory_Orya', 'pag_Latn', 'pan_Guru', 'pap_Latn', 'pol_Latn', 'por_Latn', 'prs_Arab', 'pbt_Arab', 'quy_Latn', 'ron_Latn', 'run_Latn', 'rus_Cyrl', 'sag_Latn', 'san_Deva', 'sat_Beng', 'scn_Latn', 'shn_Mymr', 'sin_Sinh', 'slk_Latn', 'slv_Latn', 'smo_Latn', 'sna_Latn', 'snd_Arab', 'som_Latn', 'sot_Latn', 'spa_Latn', 'als_Latn', 'srd_Latn', 'srp_Cyrl', 'ssw_Latn', 'sun_Latn', 'swe_Latn', 'swh_Latn', 'szl_Latn', 'tam_Taml', 'tat_Cyrl', 'tel_Telu', 'tgk_Cyrl', 'tgl_Latn', 'tha_Thai', 'tir_Ethi', 'taq_Latn', 'taq_Tfng', 'tpi_Latn', 'tsn_Latn', 'tso_Latn', 'tuk_Latn', 'tum_Latn', 'tur_Latn', 'twi_Latn', 'tzm_Tfng', 'uig_Arab', 'ukr_Cyrl', 'umb_Latn', 'urd_Arab', 'uzn_Latn', 'vec_Latn', 'vie_Latn', 'war_Latn', 'wol_Latn', 'xho_Latn', 'ydd_Hebr', 'yor_Latn', 'yue_Hant', 'zho_Hans', 'zho_Hant', 'zul_Latn']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256001: AddedToken(\"ace_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256002: AddedToken(\"ace_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256003: AddedToken(\"acm_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256004: AddedToken(\"acq_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256005: AddedToken(\"aeb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256006: AddedToken(\"afr_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256007: AddedToken(\"ajp_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256008: AddedToken(\"aka_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256009: AddedToken(\"amh_Ethi\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256010: AddedToken(\"apc_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256011: AddedToken(\"arb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256012: AddedToken(\"ars_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256013: AddedToken(\"ary_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256014: AddedToken(\"arz_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256015: AddedToken(\"asm_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256016: AddedToken(\"ast_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256017: AddedToken(\"awa_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256018: AddedToken(\"ayr_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256019: AddedToken(\"azb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256020: AddedToken(\"azj_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256021: AddedToken(\"bak_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256022: AddedToken(\"bam_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256023: AddedToken(\"ban_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256024: AddedToken(\"bel_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256025: AddedToken(\"bem_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256026: AddedToken(\"ben_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256027: AddedToken(\"bho_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256028: AddedToken(\"bjn_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256029: AddedToken(\"bjn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256030: AddedToken(\"bod_Tibt\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256031: AddedToken(\"bos_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256032: AddedToken(\"bug_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256033: AddedToken(\"bul_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256034: AddedToken(\"cat_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256035: AddedToken(\"ceb_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256036: AddedToken(\"ces_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256037: AddedToken(\"cjk_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256038: AddedToken(\"ckb_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256039: AddedToken(\"crh_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256040: AddedToken(\"cym_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256041: AddedToken(\"dan_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256042: AddedToken(\"deu_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256043: AddedToken(\"dik_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256044: AddedToken(\"dyu_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256045: AddedToken(\"dzo_Tibt\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256046: AddedToken(\"ell_Grek\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256047: AddedToken(\"eng_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256048: AddedToken(\"epo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256049: AddedToken(\"est_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256050: AddedToken(\"eus_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256051: AddedToken(\"ewe_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256052: AddedToken(\"fao_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256053: AddedToken(\"pes_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256054: AddedToken(\"fij_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256055: AddedToken(\"fin_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256056: AddedToken(\"fon_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256057: AddedToken(\"fra_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256058: AddedToken(\"fur_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256059: AddedToken(\"fuv_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256060: AddedToken(\"gla_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256061: AddedToken(\"gle_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256062: AddedToken(\"glg_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256063: AddedToken(\"grn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256064: AddedToken(\"guj_Gujr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256065: AddedToken(\"hat_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256066: AddedToken(\"hau_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256067: AddedToken(\"heb_Hebr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256068: AddedToken(\"hin_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256069: AddedToken(\"hne_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256070: AddedToken(\"hrv_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256071: AddedToken(\"hun_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256072: AddedToken(\"hye_Armn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256073: AddedToken(\"ibo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256074: AddedToken(\"ilo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256075: AddedToken(\"ind_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256076: AddedToken(\"isl_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256077: AddedToken(\"ita_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256078: AddedToken(\"jav_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256079: AddedToken(\"jpn_Jpan\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256080: AddedToken(\"kab_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256081: AddedToken(\"kac_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256082: AddedToken(\"kam_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256083: AddedToken(\"kan_Knda\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256084: AddedToken(\"kas_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256085: AddedToken(\"kas_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256086: AddedToken(\"kat_Geor\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256087: AddedToken(\"knc_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256088: AddedToken(\"knc_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256089: AddedToken(\"kaz_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256090: AddedToken(\"kbp_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256091: AddedToken(\"kea_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256092: AddedToken(\"khm_Khmr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256093: AddedToken(\"kik_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256094: AddedToken(\"kin_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256095: AddedToken(\"kir_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256096: AddedToken(\"kmb_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256097: AddedToken(\"kon_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256098: AddedToken(\"kor_Hang\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256099: AddedToken(\"kmr_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256100: AddedToken(\"lao_Laoo\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256101: AddedToken(\"lvs_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256102: AddedToken(\"lij_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256103: AddedToken(\"lim_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256104: AddedToken(\"lin_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256105: AddedToken(\"lit_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256106: AddedToken(\"lmo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256107: AddedToken(\"ltg_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256108: AddedToken(\"ltz_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256109: AddedToken(\"lua_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256110: AddedToken(\"lug_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256111: AddedToken(\"luo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256112: AddedToken(\"lus_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256113: AddedToken(\"mag_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256114: AddedToken(\"mai_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256115: AddedToken(\"mal_Mlym\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256116: AddedToken(\"mar_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256117: AddedToken(\"min_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256118: AddedToken(\"mkd_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256119: AddedToken(\"plt_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256120: AddedToken(\"mlt_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256121: AddedToken(\"mni_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256122: AddedToken(\"khk_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256123: AddedToken(\"mos_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256124: AddedToken(\"mri_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256125: AddedToken(\"zsm_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256126: AddedToken(\"mya_Mymr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256127: AddedToken(\"nld_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256128: AddedToken(\"nno_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256129: AddedToken(\"nob_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256130: AddedToken(\"npi_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256131: AddedToken(\"nso_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256132: AddedToken(\"nus_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256133: AddedToken(\"nya_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256134: AddedToken(\"oci_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256135: AddedToken(\"gaz_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256136: AddedToken(\"ory_Orya\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256137: AddedToken(\"pag_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256138: AddedToken(\"pan_Guru\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256139: AddedToken(\"pap_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256140: AddedToken(\"pol_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256141: AddedToken(\"por_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256142: AddedToken(\"prs_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256143: AddedToken(\"pbt_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256144: AddedToken(\"quy_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256145: AddedToken(\"ron_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256146: AddedToken(\"run_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256147: AddedToken(\"rus_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256148: AddedToken(\"sag_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256149: AddedToken(\"san_Deva\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256150: AddedToken(\"sat_Beng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256151: AddedToken(\"scn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256152: AddedToken(\"shn_Mymr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256153: AddedToken(\"sin_Sinh\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256154: AddedToken(\"slk_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256155: AddedToken(\"slv_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256156: AddedToken(\"smo_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256157: AddedToken(\"sna_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256158: AddedToken(\"snd_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256159: AddedToken(\"som_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256160: AddedToken(\"sot_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256161: AddedToken(\"spa_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256162: AddedToken(\"als_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256163: AddedToken(\"srd_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256164: AddedToken(\"srp_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256165: AddedToken(\"ssw_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256166: AddedToken(\"sun_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256167: AddedToken(\"swe_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256168: AddedToken(\"swh_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256169: AddedToken(\"szl_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256170: AddedToken(\"tam_Taml\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256171: AddedToken(\"tat_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256172: AddedToken(\"tel_Telu\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256173: AddedToken(\"tgk_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256174: AddedToken(\"tgl_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256175: AddedToken(\"tha_Thai\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256176: AddedToken(\"tir_Ethi\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256177: AddedToken(\"taq_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256178: AddedToken(\"taq_Tfng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256179: AddedToken(\"tpi_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256180: AddedToken(\"tsn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256181: AddedToken(\"tso_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256182: AddedToken(\"tuk_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256183: AddedToken(\"tum_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256184: AddedToken(\"tur_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256185: AddedToken(\"twi_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256186: AddedToken(\"tzm_Tfng\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256187: AddedToken(\"uig_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256188: AddedToken(\"ukr_Cyrl\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256189: AddedToken(\"umb_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256190: AddedToken(\"urd_Arab\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256191: AddedToken(\"uzn_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256192: AddedToken(\"vec_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256193: AddedToken(\"vie_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256194: AddedToken(\"war_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256195: AddedToken(\"wol_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256196: AddedToken(\"xho_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256197: AddedToken(\"ydd_Hebr\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256198: AddedToken(\"yor_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256199: AddedToken(\"yue_Hant\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256200: AddedToken(\"zho_Hans\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256201: AddedToken(\"zho_Hant\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256202: AddedToken(\"zul_Latn\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t256203: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m en_sentence = split_datasets[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m66\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtranslation\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m es_sentence = split_datasets[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m66\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtranslation\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mes_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2879\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2877\u001b[39m     encodings = \u001b[38;5;28mself\u001b[39m._call_one(text=text, text_pair=text_pair, **all_kwargs)\n\u001b[32m   2878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2879\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_switch_to_target_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2880\u001b[39m     target_encodings = \u001b[38;5;28mself\u001b[39m._call_one(text=text_target, text_pair=text_pair_target, **all_kwargs)\n\u001b[32m   2881\u001b[39m \u001b[38;5;66;03m# Leave back tokenizer in input mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\nllb\\tokenization_nllb_fast.py:264\u001b[39m, in \u001b[36mNllbTokenizerFast._switch_to_target_mode\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_switch_to_target_mode\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_tgt_lang_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\nllb\\tokenization_nllb_fast.py:294\u001b[39m, in \u001b[36mNllbTokenizerFast.set_tgt_lang_special_tokens\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_tgt_lang_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    290\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reset the special tokens to the target lang setting.\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[33;03m    - In legacy mode: No prefix and suffix=[eos, tgt_lang_code].\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    - In default mode: Prefix=[tgt_lang_code], suffix = [eos]\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28mself\u001b[39m.cur_lang_code = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy_behaviour:\n\u001b[32m    296\u001b[39m         \u001b[38;5;28mself\u001b[39m.prefix_tokens = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\jupyterlab-desktop\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:356\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.convert_tokens_to_ids\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._convert_token_to_id_with_added_voc(tokens)\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][66][\"translation\"][\"en\"]\n",
    "es_sentence = split_datasets[\"train\"][66][\"translation\"][\"es\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=es_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁diɲɛ', 'er', '▁से', 'an', '▁ஐ', 'ski', '<unk>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40015e8e461c45d4b0acb2baf55fa62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, BLEU score: 0.00\n",
      "torch.Size([32, 110, 65001])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "evaluated_samples = 0\n",
    "num_samples = 25\n",
    "all_logits = []\n",
    "for batch in tqdm(eval_dataloader):\n",
    "    if evaluated_samples >= num_samples:\n",
    "        break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        all_logits.append(logits)\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    # Necessary to pad predictions and labels for being gathered\n",
    "    generated_tokens = accelerator.pad_across_processes(\n",
    "        generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "    )\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "    predictions_gathered = accelerator.gather(generated_tokens)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    evaluated_samples += len(batch[\"input_ids\"])\n",
    "\n",
    "results = metric.compute()\n",
    "print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "all_logits = torch.cat(all_logits, dim=0)\n",
    "print(all_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.4986e+00,  5.7906e-02,  2.7258e+00,  ..., -1.1224e+00,\n",
       "          -9.5362e-01, -1.4607e+00],\n",
       "         [ 6.5390e+00, -7.7212e-02,  2.7722e+00,  ..., -1.0668e+00,\n",
       "          -9.4923e-01, -1.4589e+00],\n",
       "         [ 6.6249e+00,  8.3936e-02,  2.7289e+00,  ..., -1.0488e+00,\n",
       "          -9.2998e-01, -1.4554e+00],\n",
       "         ...,\n",
       "         [ 6.7025e+00,  1.2496e-01,  2.7672e+00,  ..., -1.2927e+00,\n",
       "          -8.6860e-01, -1.4546e+00],\n",
       "         [ 6.7182e+00,  1.0310e-01,  2.8779e+00,  ..., -1.1571e+00,\n",
       "          -9.8894e-01, -1.4538e+00],\n",
       "         [ 6.7133e+00,  4.0660e-02,  2.8156e+00,  ..., -1.0568e+00,\n",
       "          -9.8556e-01, -1.4508e+00]],\n",
       "\n",
       "        [[ 6.4081e+00, -1.8062e-02,  2.7849e+00,  ..., -1.0841e+00,\n",
       "          -9.3041e-01, -1.4581e+00],\n",
       "         [ 6.4542e+00,  7.2807e-02,  2.7681e+00,  ..., -1.2727e+00,\n",
       "          -1.1708e+00, -1.4649e+00],\n",
       "         [ 6.5058e+00, -1.0938e-02,  2.8437e+00,  ..., -1.0158e+00,\n",
       "          -9.3117e-01, -1.4612e+00],\n",
       "         ...,\n",
       "         [ 6.5045e+00,  6.8257e-02,  2.7963e+00,  ..., -1.1297e+00,\n",
       "          -1.1378e+00, -1.4466e+00],\n",
       "         [ 6.5986e+00,  1.0283e-02,  2.9274e+00,  ..., -1.2750e+00,\n",
       "          -8.6012e-01, -1.4561e+00],\n",
       "         [ 6.5561e+00,  7.4860e-02,  2.8546e+00,  ..., -1.2538e+00,\n",
       "          -1.0269e+00, -1.4571e+00]],\n",
       "\n",
       "        [[ 5.1218e+00,  2.7231e-02,  3.2609e+00,  ..., -1.3340e+00,\n",
       "          -7.3223e-01, -1.4151e+00],\n",
       "         [ 5.0722e+00,  1.6013e-01,  3.1353e+00,  ..., -1.2031e+00,\n",
       "          -7.4888e-01, -1.4240e+00],\n",
       "         [ 5.3582e+00,  5.1239e-02,  3.2939e+00,  ..., -1.2908e+00,\n",
       "          -8.2749e-01, -1.4248e+00],\n",
       "         ...,\n",
       "         [ 5.2914e+00,  8.7815e-02,  3.3501e+00,  ..., -1.4293e+00,\n",
       "          -7.6734e-01, -1.4126e+00],\n",
       "         [ 5.2990e+00,  6.8600e-02,  3.4005e+00,  ..., -1.4031e+00,\n",
       "          -9.5931e-01, -1.4173e+00],\n",
       "         [ 5.3166e+00,  1.1538e-01,  3.1872e+00,  ..., -1.3588e+00,\n",
       "          -8.6499e-01, -1.4172e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.9342e+00,  1.3915e-02,  3.2134e+00,  ..., -1.2155e+00,\n",
       "          -7.9688e-01, -1.4191e+00],\n",
       "         [ 5.1450e+00,  8.8366e-02,  3.2293e+00,  ..., -1.2482e+00,\n",
       "          -7.7331e-01, -1.4261e+00],\n",
       "         [ 5.1951e+00,  1.3121e-01,  3.1966e+00,  ..., -1.2118e+00,\n",
       "          -8.7708e-01, -1.4254e+00],\n",
       "         ...,\n",
       "         [ 4.9600e+00,  4.7282e-02,  3.2010e+00,  ..., -1.3208e+00,\n",
       "          -8.2822e-01, -1.4102e+00],\n",
       "         [ 5.2392e+00,  1.2515e-01,  3.4469e+00,  ..., -1.4401e+00,\n",
       "          -7.7256e-01, -1.4144e+00],\n",
       "         [ 5.3730e+00,  1.2632e-01,  3.2321e+00,  ..., -1.4312e+00,\n",
       "          -9.0867e-01, -1.4265e+00]],\n",
       "\n",
       "        [[ 5.4206e+00,  1.3715e-01,  3.1375e+00,  ..., -1.1227e+00,\n",
       "          -9.5545e-01, -1.4334e+00],\n",
       "         [ 5.4192e+00,  8.0815e-02,  3.1504e+00,  ..., -1.0970e+00,\n",
       "          -8.0471e-01, -1.4346e+00],\n",
       "         [ 5.6232e+00,  1.2486e-01,  3.1240e+00,  ..., -1.1903e+00,\n",
       "          -8.2150e-01, -1.4426e+00],\n",
       "         ...,\n",
       "         [ 5.4957e+00,  8.6428e-02,  3.2802e+00,  ..., -1.3968e+00,\n",
       "          -8.4987e-01, -1.4319e+00],\n",
       "         [ 5.6626e+00,  1.2374e-01,  3.2752e+00,  ..., -1.3197e+00,\n",
       "          -8.6525e-01, -1.4337e+00],\n",
       "         [ 5.7144e+00,  8.8408e-02,  3.2058e+00,  ..., -1.2805e+00,\n",
       "          -9.6820e-01, -1.4281e+00]],\n",
       "\n",
       "        [[ 7.4667e+00,  8.2742e-04,  2.3656e+00,  ..., -9.7410e-01,\n",
       "          -1.0765e+00, -1.4524e+00],\n",
       "         [ 7.4020e+00,  8.0554e-02,  2.5122e+00,  ..., -8.6458e-01,\n",
       "          -1.0130e+00, -1.4468e+00],\n",
       "         [ 7.4981e+00, -3.6807e-02,  2.5872e+00,  ..., -8.9683e-01,\n",
       "          -1.0015e+00, -1.4454e+00],\n",
       "         ...,\n",
       "         [ 7.4521e+00,  2.6332e-01,  2.6101e+00,  ..., -1.1182e+00,\n",
       "          -1.1442e+00, -1.4447e+00],\n",
       "         [ 7.5320e+00,  1.1767e-01,  2.4423e+00,  ..., -1.1481e+00,\n",
       "          -1.0871e+00, -1.4397e+00],\n",
       "         [ 7.6505e+00,  1.4916e-01,  2.5660e+00,  ..., -1.0645e+00,\n",
       "          -1.0681e+00, -1.4301e+00]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remnants of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46985, 46985, 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [line.strip() for line in open('X.txt').readlines()]\n",
    "y = train_data = [int(line.strip()) for line in open('YL1.txt').readlines()]\n",
    "\n",
    "len(X), len(y), max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46000, 46000, 985, 985)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = X[:46000]\n",
    "train_y = np.array(y[:46000])\n",
    "test_X = X[46000:]\n",
    "test_y = np.array(y[46000:])\n",
    "\n",
    "len(train_X), len(train_y), len(test_X), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not needed for training or evaluation, but useful for mapping examples\n",
    "labels = {\n",
    "    0:'Computer Science',\n",
    "    1:'Electrical Engineering',\n",
    "    2:'Psychology',\n",
    "    3:'Mechanical Engineering',\n",
    "    4:'Civil Engineering',\n",
    "    5:'Medical Science',\n",
    "    6:'Biochemistry'\n",
    "}\n",
    "\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsuWp8K1tnEv"
   },
   "source": [
    "#### Torch Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfNDmY7XtnEv"
   },
   "source": [
    "* takes in inputs and outputs/labels\n",
    "* interfaces with tokenizer\n",
    "* handles batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zgsXkSJ7tnEv"
   },
   "outputs": [],
   "source": [
    "class MultiLabelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = text\n",
    "        self.targets = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            # 'targets': torch.tensor(self.targets[index], dtype=torch.long) # was float\n",
    "            'targets': self.targets[index].clone().detach()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPo-cm-LtnEw"
   },
   "source": [
    "#### BERT Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61uWksm8tnEw"
   },
   "source": [
    "* first layer is pretrained BERT model\n",
    "* add whatever layers after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TUgd_9egQ5Ok"
   },
   "outputs": [],
   "source": [
    "class ELECTRAClass(torch.nn.Module):\n",
    "    def __init__(self, NUM_OUT):\n",
    "        # super(BERTClass, self).__init__()\n",
    "        super(ELECTRAClass, self).__init__()\n",
    "        # self.l1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.electra = ElectraForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\", num_labels=NUM_OUT)\n",
    "\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 256)\n",
    "        # self.classifier = torch.nn.Linear(768, NUM_OUT)\n",
    "#         self.dropout = torch.nn.Dropout(0.5)\n",
    "        # self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#         output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "# #         pooler = self.pre_classifier(pooler)\n",
    "# #         pooler = torch.nn.Tanh()(pooler)\n",
    "# #         pooler = self.dropout(pooler)\n",
    "#         output = self.classifier(pooler)\n",
    "#         output = self.softmax(output)\n",
    "        # return output\n",
    "\n",
    "        output = self.electra(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        return output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5Xqdrb8tnEw"
   },
   "source": [
    "#### Helpful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A0PsiOztnEw"
   },
   "source": [
    "Loss\n",
    "* Tasks with only two labels use binary crossentropy\n",
    "* Tasks with more labels will use categorical crossentropy\n",
    "* Tasks that don't have labels, but rather have distributions should use KL divergence\n",
    "* Tasks that don't have distributions should use something like RMSE loss\n",
    "\n",
    "Train\n",
    "* Steps through the data batch by batch\n",
    "* grabs ids, masks, and token_type_ids which are required inputs for BERT\n",
    "* inputs are passed through the model, compared to targets, computes loss function, backprops\n",
    "\n",
    "Validation\n",
    "* Takes a model, passes inputs\n",
    "* Need to use the targets from here because they are potentially shuffled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "3itaIpBptnEw"
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    # return torch.nn.BCELoss()(outputs, targets)\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets) # not likely to work on first go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5skzQ1A6tnEx"
   },
   "outputs": [],
   "source": [
    "def train(model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long) # was float\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "g6lAPrIMtnEx"
   },
   "outputs": [],
   "source": [
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testing_loader):\n",
    "            targets = data['targets'].to(device, dtype=torch.long)\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = torch.argmax(outputs, dim=1).cpu().detach()\n",
    "            # outputs = torch.sigmoid(outputs).cpu().detach()\n",
    "            fin_outputs.extend(outputs)\n",
    "            fin_targets.extend(targets)\n",
    "    return torch.stack(fin_outputs), torch.stack(fin_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RteD_-7tnEx"
   },
   "source": [
    "#### The Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27oiPC3ftnEx"
   },
   "source": [
    "* converts raw string to ids, masks, and token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495,
     "referenced_widgets": [
      "32db26cc5ebf44f2afdf18f34a04339c",
      "a9d081154ce14ced9d4417c69431c4fd",
      "e97e7dd68c7d4746bea4b2d09f66426b",
      "b3b15e6e4cb7475980ef3edbad6b69a2",
      "b9589ce7f514465685aa11f5eb8ca619",
      "4ae00f58216c42e8b46b712a64587d3d",
      "fcfd71b6fb09464bbb16196b45899a9e",
      "5a008741128d44838f7fa15bc9a64707",
      "7f0b797e6b474ffebc66868fda26e6a6",
      "6c85e6345c78441e87acaaebaccfc8ac",
      "38ebfa128ebe4905b86204776cf0b091",
      "57f68f44db6e4c219faaf9b1eb19542b",
      "b45b84890ea34eee9660df18e4c6ae1c",
      "72c50e1fc893476fb042e3665a51c725",
      "5e309029aebf444db8a5e2313533394f",
      "c2b58c3bfeb941759f34b3312268fd16",
      "ea2384a2c28f4f84b4763ce5a0886d3d",
      "8ce2c25b7eae4cda818066c8a32efa4e",
      "5bd7943f1fd64012a821364c3a9025eb",
      "30937209a91c4aa89b763b74ba004e78",
      "0384665c6f89482eaf75d0e5b0944532",
      "b497a72d4a994cd1ab87d8f4dfb39601",
      "c4dd18c5e99c4ee39f99ab22c88480d1",
      "6cb9b15f8a634a2ea605dde4ebc4fe13",
      "d614ce6c6aa347e09717b27f12608d2f",
      "5cb98bf9ded442d1b64550b5133ae1ff",
      "9315cf0e515d446e98fb264ce28297a5",
      "5d1130c0782243908d01c9c1fc9cb97c",
      "70a519a8c6ae497990fc4ea864da159d",
      "7596ba6378e94438a90df8e668f659cf",
      "b0bb637615564f65b4246ae4eca99a57",
      "25ab14ac53f94591b516a6bdc34c6fcd",
      "005148cfbda24c1380bf802f38c900ff",
      "43b844f8941d40cd917107df0d951769",
      "9b90ca8cb54440eb91f440ad4c1e133b",
      "c436625619aa48c493004718614b33fd",
      "161527dacde2497a90f6eec0c1cc1018",
      "3e0e90f0744a4caabedb48c3bd82da71",
      "a500ee9befb640a8818be266284c2996",
      "f8349c4370464f1da185076dc3b8b2c0",
      "c7251c84236749ec903f04890eb73704",
      "de2bf12d512c42e797960ebd8564649b",
      "151d6305a1984981b00cabb9d2a08ec7",
      "3e8baf1fff664ca898e46c9b5b26cf14"
     ]
    },
    "id": "zDQAfHaotnEx",
    "outputId": "cf2a2798-db4c-46f8-f93a-db9c987ea263",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32db26cc5ebf44f2afdf18f34a04339c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f68f44db6e4c219faaf9b1eb19542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dd18c5e99c4ee39f99ab22c88480d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b844f8941d40cd917107df0d951769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (Objective) In order to increase classification accuracy of tea-category identification (TCI) system, this paper proposed a novel approach. (Method) The proposed methods first extracted 64 color histogram to obtain color information, and 16 wavelet packet entropy to obtain the texture information. With the aim of reducing the 80 features, principal component analysis was harnessed. The reduced features were used as input to generalized eigenvalue proximal support vector machine (GEPSVM). Winner-takes-all (WTA) was used to handle the multiclass problem. Two kernels were tested, linear kernel and Radial basis function (RBF) kernel. Ten repetitions of 10-fold stratified cross validation technique were used to estimate the out-of-sample errors. We named our method as GEPSVM + RBF + WTA and GEPSVM + WTA. (Result) The results showed that PCA reduced the 80 features to merely five with explaining 99.90% of total variance. The recall rate of GEPSVM + RBF + WTA achieved the highest overall recall rate of 97.9%. (Conclusion) This was higher than the result of GEPSVM + WTA and other five state-of-the-art algorithms: back propagation neural network, RBF support vector machine, genetic neural-network, linear discriminant analysis, and fitness-scaling chaotic artificial bee colony artificial neural network.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1006, 7863, 1007, 1999, 2344, 2000, 3623, 5579, 10640, 1997, 5572, 1011, 4696, 8720, 1006, 22975, 2072, 1007, 2291, 1010, 2023, 3259, 3818, 1037, 3117, 3921, 1012, 1006, 4118, 1007, 1996, 3818, 4725, 2034, 15901, 4185, 3609, 2010, 3406, 13113, 2000, 6855, 3609, 2592, 1010, 1998, 2385, 4400, 7485, 14771, 23077, 2000, 6855, 1996, 14902, 2592, 1012, 2007, 1996, 6614, 1997, 8161, 1996, 3770, 2838, 1010, 4054, 6922, 4106, 2001, 17445, 2098, 1012, 1996, 4359, 2838, 2020, 2109, 2004, 7953, 2000, 18960, 1041, 29206, 10175, 5657, 4013, 9048, 9067, 2490, 9207, 3698, 1006, 16216, 4523, 2615, 2213, 1007, 1012, 3453, 1011, 3138, 1011, 2035, 1006, 21925, 1007, 2001, 2109, 2000, 5047, 1996, 4800, 26266, 3291, 1012, 2048, 16293, 2015, 2020, 7718, 1010, 7399, 16293, 1998, 15255, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained('google/electra-small-discriminator')\n",
    "\n",
    "# what does the tokenizer do?\n",
    "print(f\"train data: {train_X[5]}\")\n",
    "\n",
    "tokenizer.encode_plus(\n",
    "            train_X[5],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            # pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFkAQso0tnEx"
   },
   "source": [
    "#### Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R9k7Z3DtnEy"
   },
   "source": [
    "* hyperparameters\n",
    "* setup dataset\n",
    "* setup parameters\n",
    "* setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Rn4x_vAgtnEy"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64 #original: 64\n",
    "EPOCHS = 3\n",
    "NUM_OUT = len(labels)\n",
    "LEARNING_RATE = 2e-05\n",
    "\n",
    "training_data = MultiLabelDataset(train_X, torch.from_numpy(train_y), tokenizer, MAX_LEN)\n",
    "test_data = MultiLabelDataset(test_X, torch.from_numpy(test_y), tokenizer, MAX_LEN)\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 2\n",
    "                }\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, **train_params)\n",
    "testing_loader = torch.utils.data.DataLoader(test_data, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp-1_--AtnEy"
   },
   "source": [
    "#### Train, Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRs0LELBtnEy"
   },
   "source": [
    "* model.to -> send to GPU, if available (anything computed should be put onto the GPU)\n",
    "* setup optimizer - could use Stochastic Gradient Descent, but ADAM tends to work better\n",
    "* for each epoch, train, show the loss, evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 813,
     "referenced_widgets": [
      "7679c467b7c24faaaca7550021f212d5",
      "e84cc9b5e4a9456da637b0d819c29269",
      "11d49fea374243a2a7b10d0db04afa47",
      "09b7de4941664a92a640848833c0724d",
      "d58ccded6d054d9ba2abdc36463c7109",
      "4d218c2beafb4e2f9e079545b09bd925",
      "04ceb8d0d9c24adf888336bb1aa407d4",
      "a2d045478f324303be4997d0840dc733",
      "31e8a2b547ae4ed586c5b0b2951c59eb",
      "600270f338424a939a1ef6ea1977c3d2",
      "105a3a2c40214c099b17b9d6f0ec341b",
      "eb7a5a9dd6e24694bad3ef7f059faea9",
      "80c6e3ee47e0485eb72f7d170c932935",
      "4a4a03ce036742b2b1b26c62ce830166",
      "c200e334d60047aabf2bf1c6d516c52f",
      "1f3f5ffd75184fa89c43cb634785e096",
      "2ebd0b5f420e43c9acc45e20f86b89d9",
      "9ed8e8173a77410d94ea86ae08ab6086",
      "0d0a87a1efe94c8491f90a87658c2f7e",
      "8993a7a17d594f59a762ec235030775f",
      "2b3e664b5785426a8a339aaabdb51efc",
      "01230bce3cf744cdbe5718d93796f8dd"
     ]
    },
    "id": "cJe__ABKtnEy",
    "outputId": "d8d9ca29-8fbe-4476-daff-6011ed085397"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7679c467b7c24faaaca7550021f212d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/719 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7a5a9dd6e24694bad3ef7f059faea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/54.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 719/719 [05:47<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.889681339263916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy on test set 0.7269035532994924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/719 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 719/719 [06:22<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.6426712870597839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy on test set 0.7857868020304568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/719 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 719/719 [06:22<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  0.5695027709007263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy on test set 0.8060913705583757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = BERTClass(NUM_OUT)\n",
    "model = ELECTRAClass(NUM_OUT)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, training_loader, optimizer)\n",
    "    print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "    guess, targs = validation(model, testing_loader)\n",
    "    # guesses = torch.max(guess, dim=1)\n",
    "    guesses = guess.cpu()\n",
    "    # targets = torch.max(targs, dim=1)\n",
    "    targets = targs.cpu()\n",
    "    # print('arracy on test set {}'.format(accuracy_score(guesses.indices, targets.indices)))\n",
    "    print(\"\\naccuracy on test set {}\".format(accuracy_score(guesses, targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz3xBEZYH6Sm"
   },
   "outputs": [],
   "source": [
    "# terminate this colab session to release resources\n",
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "0xBVqJ7UHRgk"
   ],
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
